"""
Pendulum è¨“ç·´ä¸»ç¨‹å¼
* ç¤ºç¯„ DDPG å’Œ TD3 çš„è¨“ç·´æµç¨‹
* å±•ç¾ Polymorphismï¼šä¸åŒ Agent ä½¿ç”¨ç›¸åŒä»‹é¢
* åŒ…å«è¨“ç·´æ›²ç·šè¨˜éŒ„å’Œæ¨¡å‹å„²å­˜
"""

import numpy as np
import matplotlib.pyplot as plt
from manage import PendulumEnvWrapper, Experiment
from Agents.DDPG_agent import DDPG_Agent
from Agents.TD3_agent import TD3_Agent
import argparse
import os
from datetime import datetime


def train_agent(agent_type: str = "TD3", 
                total_episodes: int = 100,
                warmup_steps: int = 1000,
                save_interval: int = 20,
                test_interval: int = 5):
    """
    è¨“ç·´æ™ºèƒ½é«”
    
    Args:
        agent_type: "DDPG" æˆ– "TD3"
        total_episodes: ç¸½è¨“ç·´å›åˆæ•¸
        warmup_steps: éš¨æ©Ÿæ¢ç´¢æ­¥æ•¸ï¼ˆå¡«å……ç·©è¡å€ï¼‰
        save_interval: å„²å­˜æ¨¡å‹çš„é–“éš”
        test_interval: æ¸¬è©¦è©•ä¼°çš„é–“éš”
    """
    # å»ºç«‹ç’°å¢ƒ
    env_wrapper = PendulumEnvWrapper(render_mode=None)
    
    # å»ºç«‹æ™ºèƒ½é«” (Polymorphism - ä½¿ç”¨ç›¸åŒä»‹é¢ä½†ä¸åŒå¯¦ä½œ)
    if agent_type == "DDPG":
        agent = DDPG_Agent(
            action=env_wrapper.action,
            max_action=env_wrapper.max_action,
            state_dim=env_wrapper.state,
            gamma=0.99,
            tau=0.005,
            actor_lr=1e-3,
            critic_lr=1e-3,
            buffer_size=50000,
            batch_size=256,
            exploration_noise=0.1
        )
    elif agent_type == "TD3":
        agent = TD3_Agent(
            action=env_wrapper.action,
            max_action=env_wrapper.max_action,
            state_dim=env_wrapper.state,
            gamma=0.99,
            tau=0.005,
            actor_lr=1e-3,
            critic_lr=1e-3,
            buffer_size=50000,
            batch_size=256,
            exploration_noise=0.1,
            policy_noise=0.2,
            noise_clip=0.5,
            policy_delay=2
        )
    else:
        raise ValueError(f"Unknown agent type: {agent_type}")
    
    # å»ºç«‹å¯¦é©—ç®¡ç†å™¨
    experiment = Experiment(env=env_wrapper, agent=agent, episode_len=200)
    
    # å»ºç«‹å„²å­˜ç›®éŒ„
    # timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    # save_dir = f"results/{agent_type}_{timestamp}"    #é€™å€‹æœƒæœ‰æ™‚é–“æ¨™è¨˜
    save_dir = f"results/{agent_type}"
    os.makedirs(save_dir, exist_ok=True)
    
    # è¨˜éŒ„è¨“ç·´æ•¸æ“š
    train_rewards = []
    test_rewards = []
    critic_losses = []
    actor_losses = []
    
    print(f"\n{'='*60}")
    print(f"é–‹å§‹è¨“ç·´ {agent_type} Agent")
    print(f"ç¸½å›åˆæ•¸: {total_episodes}, æš–èº«æ­¥æ•¸: {warmup_steps}")
    print(f"å„²å­˜ç›®éŒ„: {save_dir}")
    print(f"{'='*60}\n")
    
    # ==================== æš–èº«éšæ®µï¼ˆéš¨æ©Ÿæ¢ç´¢ï¼‰ ====================
    print("Phase 1: æš–èº«éšæ®µ - éš¨æ©Ÿæ¢ç´¢æ”¶é›†åˆå§‹ç¶“é©—...")
    obs = env_wrapper.reset()
    for step in range(warmup_steps):
        # éš¨æ©Ÿå‹•ä½œ
        action = np.random.uniform(-env_wrapper.max_action, 
                                  env_wrapper.max_action, 
                                  size=env_wrapper.action)
        next_obs, reward, done, _ = env_wrapper.step(action)
        
        # å„²å­˜ç¶“é©—
        agent.store_transition(obs, action, reward, next_obs, done)
        
        obs = next_obs if not done else env_wrapper.reset()
        
        if (step + 1) % 100 == 0:
            print(f"  æš–èº«é€²åº¦: {step + 1}/{warmup_steps}")
    
    print(f"âœ“ æš–èº«å®Œæˆï¼ç·©è¡å€å¤§å°: {len(agent.replay_buffer)}\n")
    
    # ==================== è¨“ç·´éšæ®µ ====================
    print("Phase 2: è¨“ç·´éšæ®µ")
    best_reward = -np.inf
    
    for episode in range(1, total_episodes + 1):
        # è¨“ç·´ä¸€å€‹ episode
        total_reward, avg_critic_loss, avg_actor_loss = experiment.train_episode()
        
        # è¨˜éŒ„æ•¸æ“š
        train_rewards.append(total_reward)
        critic_losses.append(avg_critic_loss)
        actor_losses.append(avg_actor_loss)
        
        # æ¯éš”ä¸€æ®µæ™‚é–“é€²è¡Œæ¸¬è©¦è©•ä¼°
        if episode % test_interval == 0:
            agent.set_training_mode(False)  # åˆ‡æ›åˆ°æ¸¬è©¦æ¨¡å¼
            test_reward = experiment.run_episode(render=False)
            test_rewards.append(test_reward)
            agent.set_training_mode(True)   # åˆ‡æ›å›è¨“ç·´æ¨¡å¼
            
            print(f"Episode {episode:3d} | "
                  f"Train: {total_reward:7.2f} | "
                  f"Test: {test_reward:7.2f} | "
                  f"Critic Loss: {avg_critic_loss:6.3f} | "
                  f"Actor Loss: {avg_actor_loss:6.3f}")
            
            # å„²å­˜æœ€ä½³æ¨¡å‹
            if test_reward > best_reward:
                best_reward = test_reward
                agent.save(f"{save_dir}/best_model.pth")
                print(f"  â†’ ğŸ† æ–°æœ€ä½³æ¨¡å‹ï¼çå‹µ: {best_reward:.2f}")
        
        # å®šæœŸå„²å­˜æª¢æŸ¥é»
        if episode % save_interval == 0:
            agent.save(f"{save_dir}/checkpoint_ep{episode}.pth")
    
    # å„²å­˜æœ€çµ‚æ¨¡å‹
    agent.save(f"{save_dir}/final_model.pth")
    
    # ==================== ç¹ªè£½è¨“ç·´æ›²ç·š ====================
    print("\nç¹ªè£½è¨“ç·´æ›²ç·š...")
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    
    # è¨“ç·´çå‹µ
    axes[0, 0].plot(train_rewards, alpha=0.6, label='Episode Reward')
    axes[0, 0].plot(np.convolve(train_rewards, np.ones(10)/10, mode='valid'), 
                    'r-', linewidth=2, label='Moving Average (10)')
    axes[0, 0].set_xlabel('Episode')
    axes[0, 0].set_ylabel('Total Reward')
    axes[0, 0].set_title('Training Rewards')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    
    # æ¸¬è©¦çå‹µ
    test_episodes = list(range(test_interval, total_episodes + 1, test_interval))
    axes[0, 1].plot(test_episodes, test_rewards, 'go-', linewidth=2, markersize=4)
    axes[0, 1].set_xlabel('Episode')
    axes[0, 1].set_ylabel('Total Reward')
    axes[0, 1].set_title('Test Rewards')
    axes[0, 1].grid(True, alpha=0.3)
    
    # Critic Loss
    axes[1, 0].plot(critic_losses, alpha=0.8)
    axes[1, 0].set_xlabel('Episode')
    axes[1, 0].set_ylabel('Critic Loss')
    axes[1, 0].set_title('Critic Loss Curve')
    axes[1, 0].grid(True, alpha=0.3)
    
    # Actor Loss
    axes[1, 1].plot(actor_losses, alpha=0.8, color='orange')
    axes[1, 1].set_xlabel('Episode')
    axes[1, 1].set_ylabel('Actor Loss')
    axes[1, 1].set_title('Actor Loss Curve')
    axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(f"{save_dir}/training_curves.png", dpi=150)
    print(f"âœ“ è¨“ç·´æ›²ç·šå·²å„²å­˜è‡³ {save_dir}/training_curves.png")
    
    # å„²å­˜è¨“ç·´æ•¸æ“š
    np.savez(f"{save_dir}/training_data.npz",
             train_rewards=train_rewards,
             test_rewards=test_rewards,
             critic_losses=critic_losses,
             actor_losses=actor_losses)
    print(f"âœ“ è¨“ç·´æ•¸æ“šå·²å„²å­˜è‡³ {save_dir}/training_data.npz")
    
    print(f"\n{'='*60}")
    print(f"è¨“ç·´å®Œæˆï¼")
    print(f"æœ€ä½³æ¸¬è©¦çå‹µ: {best_reward:.2f}")
    print(f"æœ€çµ‚è¨“ç·´çå‹µ: {train_rewards[-1]:.2f}")
    print(f"æ¨¡å‹å·²å„²å­˜è‡³: {save_dir}")
    print(f"{'='*60}\n")
    
    experiment.close()


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Train Pendulum Agent')
    parser.add_argument('--agent', type=str, default='TD3', choices=['DDPG', 'TD3'],
                       help='Agent type (default: TD3)')
    parser.add_argument('--episodes', type=int, default=100,
                       help='Total training episodes (default: 100)')
    parser.add_argument('--warmup', type=int, default=1000,
                       help='Warmup steps for random exploration (default: 1000)')
    
    args = parser.parse_args()
    
    train_agent(
        agent_type=args.agent,
        total_episodes=args.episodes,
        warmup_steps=args.warmup
    )
