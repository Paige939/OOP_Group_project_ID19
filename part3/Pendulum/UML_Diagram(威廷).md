# Part 3 Pendulum - UML é¡åˆ¥åœ–

## Class Diagram

```mermaid
classDiagram
    direction TB
    
    %% ===== Abstract Base Class =====
    class Agent {
        <<abstract>>
        +action: int
        +max_action: float
        +__init__(action, max_action)
        +act(observation)* np.ndarray
        +reset()*
        +pre_episode(env, episode_len)
    }
    
    %% ===== Concrete Agents =====
    class RandomAgent {
        +act(observation) np.ndarray
        +reset()
    }
    
    class DDPG_Agent {
        -device: torch.device
        -actor: ActorNetwork
        -critic: CriticNetwork
        -actor_target: ActorNetwork
        -critic_target: CriticNetwork
        -replay_buffer: ReplayBuffer
        -gamma: float
        -tau: float
        -batch_size: int
        -exploration_noise: float
        -training_mode: bool
        +act(observation, add_noise) np.ndarray
        +reset()
        +store_transition(state, action, reward, next_state, done)
        +train_step() Tuple[float, float]
        +set_training_mode(mode)
        +save(filepath)
        +load(filepath)
        -_soft_update(source, target)
    }
    
    class TD3_Agent {
        -device: torch.device
        -actor: ActorNetwork
        -critic: TwinCriticNetwork
        -actor_target: ActorNetwork
        -critic_target: TwinCriticNetwork
        -replay_buffer: ReplayBuffer
        -policy_delay: int
        -policy_noise: float
        -noise_clip: float
        -total_iterations: int
        +act(observation, add_noise) np.ndarray
        +reset()
        +store_transition(state, action, reward, next_state, done)
        +train_step() Tuple[float, float]
        +set_training_mode(mode)
        +save(filepath)
        +load(filepath)
        -_soft_update(source, target)
    }
    
    class CEM_Agent {
        -num_samples: int
        -elite_frac: float
        -num_elite: int
        -weights_mean: np.ndarray
        -weights_std: np.ndarray
        -best_weights: np.ndarray
        -save_path: str
        +act(observation) np.ndarray
        +reset()
        +pre_episode(env, episode_len)
        -evaluate(env, weights, episode_len) float
    }
    
    class LQRAgent {
        -K: np.ndarray
        -g: float
        -m: float
        -l: float
        +act(observation) np.ndarray
        +reset()
    }
    
    class EnergyControlAgent {
        -g: float
        -m: float
        -l: float
        +act(observation) np.ndarray
        +reset()
        +get_energy(cos_th, sin_th, th_dot) float
    }
    
    class ELAgent {
        -swing_up_agent: EnergyControlAgent
        -balance_agent: LQRAgent
        +act(observation) np.ndarray
        +reset()
    }
    
    %% ===== Neural Networks (nn.Module) =====
    class ActorNetwork {
        -max_action: float
        -hidden: nn.Sequential
        -output_layer: nn.Linear
        +forward(state) torch.Tensor
    }
    
    class CriticNetwork {
        -hidden: nn.Sequential
        -output_layer: nn.Linear
        +forward(state, action) torch.Tensor
    }
    
    class TwinCriticNetwork {
        -q1_hidden: nn.Sequential
        -q1_output: nn.Linear
        -q2_hidden: nn.Sequential
        -q2_output: nn.Linear
        +forward(state, action) Tuple
        +Q1(state, action) torch.Tensor
    }
    
    %% ===== Replay Buffer =====
    class ReplayBuffer {
        -capacity: int
        -ptr: int
        -size: int
        -states: np.ndarray
        -actions: np.ndarray
        -rewards: np.ndarray
        -next_states: np.ndarray
        -dones: np.ndarray
        +store(state, action, reward, next_state, done)
        +sample(batch_size) Dict
        +__len__() int
    }
    
    %% ===== Environment Wrapper =====
    class PendulumEnvWrapper {
        -env: gym.Env
        -current_observation: np.ndarray
        +state: int
        +action: int
        +max_action: float
        +reset() np.ndarray
        +step(action) Tuple
        +render()
        +close()
        +get_state() np.ndarray
    }
    
    %% ===== Experiment Manager =====
    class Experiment {
        -env: PendulumEnvWrapper
        -agent: Agent
        -episode_len: int
        +run_episode(render) float
        +close()
    }
    
    %% ========== Relationships ==========
    
    %% Inheritance (ç¹¼æ‰¿)
    Agent <|-- RandomAgent : extends
    Agent <|-- DDPG_Agent : extends
    Agent <|-- TD3_Agent : extends
    Agent <|-- CEM_Agent : extends
    Agent <|-- LQRAgent : extends
    Agent <|-- EnergyControlAgent : extends
    Agent <|-- ELAgent : extends
    
    %% Composition (çµ„åˆ) - DDPG
    DDPG_Agent *-- ActorNetwork : actor
    DDPG_Agent *-- CriticNetwork : critic
    DDPG_Agent *-- ReplayBuffer : buffer
    
    %% Composition (çµ„åˆ) - TD3
    TD3_Agent *-- ActorNetwork : actor
    TD3_Agent *-- TwinCriticNetwork : twin_critic
    TD3_Agent *-- ReplayBuffer : buffer
    
    %% Composition (çµ„åˆ) - ELAgent (æ··åˆç­–ç•¥)
    ELAgent *-- EnergyControlAgent : swing_up
    ELAgent *-- LQRAgent : balance
    
    %% Aggregation (èšåˆ) - Experiment
    Experiment o-- PendulumEnvWrapper : env
    Experiment o-- Agent : agent
```

---

## å¦‚ä½•æŸ¥çœ‹ UML åœ–

### æ–¹æ³• 1ï¼šVS Code æ“´å±•
1. å®‰è£ **"Markdown Preview Mermaid Support"** æ“´å±•
2. æ‰“é–‹æ­¤æª”æ¡ˆï¼ŒæŒ‰ `Ctrl+Shift+V` é è¦½

### æ–¹æ³• 2ï¼šç·šä¸Šå·¥å…·
1. æ‰“é–‹ https://mermaid.live/
2. è¤‡è£½ä¸Šé¢çš„ mermaid ä»£ç¢¼è²¼ä¸Š
3. å³å¯çœ‹åˆ° UML åœ–

### æ–¹æ³• 3ï¼šGitHub
- ç›´æ¥æŠŠé€™å€‹ .md æª”æ¡ˆ push åˆ° GitHubï¼ŒGitHub æœƒè‡ªå‹•æ¸²æŸ“ Mermaid åœ–è¡¨

---

## ç°¡åŒ–ç‰ˆ UML

> åªé¡¯ç¤ºé¡åˆ¥åç¨±å’Œé—œä¿‚ï¼Œé©åˆå¿«é€Ÿèªªæ˜æ¶æ§‹

```mermaid
classDiagram
    direction LR
    
    %% æ ¸å¿ƒç¹¼æ‰¿é—œä¿‚
    Agent <|-- RandomAgent
    Agent <|-- DDPG_Agent
    Agent <|-- TD3_Agent
    Agent <|-- CEM_Agent
    Agent <|-- LQRAgent
    Agent <|-- EnergyControlAgent
    Agent <|-- ELAgent
    
    %% çµ„åˆé—œä¿‚
    DDPG_Agent *-- ActorNetwork
    DDPG_Agent *-- CriticNetwork
    DDPG_Agent *-- ReplayBuffer
    
    TD3_Agent *-- ActorNetwork
    TD3_Agent *-- TwinCriticNetwork
    TD3_Agent *-- ReplayBuffer
    
    ELAgent *-- EnergyControlAgent
    ELAgent *-- LQRAgent
    
    %% èšåˆé—œä¿‚
    Experiment o-- PendulumEnvWrapper
    Experiment o-- Agent
    
    %% æ¨™è¨»
    class Agent {
        <<abstract>>
    }
```

### ğŸ“Œ é—œä¿‚åœ–ä¾‹

| ç¬¦è™Ÿ | åç¨± | èªªæ˜ |
|------|------|------|
| `<\|--` | ç¹¼æ‰¿ | å­é¡åˆ¥ç¹¼æ‰¿çˆ¶é¡åˆ¥ |
| `*--` | çµ„åˆ | æ“æœ‰ï¼Œç”Ÿå‘½é€±æœŸä¸€è‡´ |
| `o--` | èšåˆ | ä½¿ç”¨ï¼Œå¯ç¨ç«‹å­˜åœ¨ |

### ğŸ“Œ ä¸€å¥è©±ç¸½çµ

> **7 ç¨® Agent ç­–ç•¥ç¹¼æ‰¿è‡ªæŠ½è±¡é¡ `Agent`ï¼Œé€éå¤šå‹å¯¦ç¾ç­–ç•¥æ¨¡å¼ï¼Œæ·±åº¦å­¸ç¿’ Agent çµ„åˆç¥ç¶“ç¶²è·¯çµ„ä»¶ã€‚**
